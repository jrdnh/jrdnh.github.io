[{"content":"The full example code for this post is here.\nA key limitation with the financial model from the last post was that line items could only reference subitems. Siblings and parents were unreachable. This post improves the hierarchical structure to make all line items accessible to each other while preserving autocompletion and type hints.\nMaking the parent accessible #As a refresher from the previous model, line items are represented by callables. Each line item may hold subitems as attributes. A simple model might look like this:\nNetIncome ├── Revenue └── Expenses └── VariableExpenses We can navigate down the tree with regular attribute access (e.g. NetIncome().revenue). We can navigate back up the tree by adding a parent attribute to the class for each line item. Since we want to avoid memory leaks where parents and children hold cyclical references to each other, we can make the reference to the parent weak and wrap it in a managed property to set and get it properly.\nWe want autocompletion to work across the entire structure, so we also need to make the node class generic with respect to its parent type. This will allow us to get good autocompletion and type hints for paths like expenses.parent.revenue navigating up to the parent and then back down to a sibling.\nimport weakref from typing import Optional from pydantic import BaseModel class Node[P](BaseModel): _parent: Optional[weakref.ref] = None @property def parent(self) -\u0026gt; P | None: \u0026#34;\u0026#34;\u0026#34;Parent node or None.\u0026#34;\u0026#34;\u0026#34; return self._parent() if self._parent is not None else None @parent.setter def parent(self, parent: Optional[P] = None): \u0026#34;\u0026#34;\u0026#34;Set parent.\u0026#34;\u0026#34;\u0026#34; self._parent = weakref.ref(parent) if parent is not None else None Note This snippet uses the syntax for generics from PEP 695 introduced in Python 3.12. For previous versions of Python use:\nfrom typing import Generic, TypeVar P = TypeVar(\u0026#34;P\u0026#34;) class Node(BaseModel, Generic[P]): ... Now we can create concrete subclasses of Node that will tell us what type the parent should be, enabling autocompletion. For example\nclass Parent(Node[None]): luftballons: int class Child(Node[Parent]): pass child = Child() child.parent = Parent(luftballons=99) child.parent.luftballons Attributes of parent receive good autocompletion, and correct type hints. Parents for arbitrarily nested children #Reproducibility for components is limited in the current structure since parent/child relationships must be strictly defined. Deeply nested children must know what the structure looks like above them in order to traverse up the right number of nodes (parent.parent.parent...).\nWe can improve navigation by adding a helper function that returns the first parent of the desired type (or raising an error if such type doesn\u0026rsquo;t exist in the parent chain).\nfrom typing import Type, TypeVar T = TypeVar(\u0026#34;T\u0026#34;, bound=\u0026#34;Node\u0026#34;) class Node[P](BaseModel): ... def find_parent(self, cls: Type[T]) -\u0026gt; T: \u0026#34;\u0026#34;\u0026#34;Find first parent node with class `cls`.\u0026#34;\u0026#34;\u0026#34; if isinstance(self.parent, cls): return self.parent # if the immediate parent doesn\u0026#39;t match, try its parent try: return self.parent.find_parent(cls) # type: ignore except AttributeError: raise ValueError(f\u0026#34;No parent of type {cls} found.\u0026#34;) Note that subclasses of cls will match and be returned in this implementation. Depending on the desired behavior, strict class comparison (or a function argument to require strict comparison) might be appropriate.\nSetting the parent attribute #It would be helpful if children nodes had their parent property automatically set during construction. Luckily, Pydantic has a post initialization hook that we can use to set the parent for any child Node attributes.\nclass Node[P](BaseModel): ... def model_post_init(self, __context: Any) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Post init hook to add self as parent to any \u0026#39;Node\u0026#39; attributes.\u0026#34;\u0026#34;\u0026#34; super().model_post_init(__context) for _, v in self: if isinstance(v, Node): v.parent = self After the model has been initialized, the hook iterates over each model attribute and sets the attribute\u0026rsquo;s parent to self if it is a Node (or subclass of a Node) object.\npydantic.BaseModel subclasses do not store attributes with a leading underscore in the object\u0026rsquo;s __dict__ (which is what is iterated over in the for-loop with Pydantic models). Instead, they are stored separately in a separate __private_attributes__ property. If the _parent attribute were stored in the object\u0026rsquo;s __dict__ (like it would be for a regular, non-BaseModel class) we would also have to make sure that we don\u0026rsquo;t try to set the parent\u0026rsquo;s parent property to itself.\nNavigating a simple model #We can now reference line items up and across the model using the Node class. A simple model that reflects the structure at the top of the post is below. Revenue is modeled as a single line. Expenses includes both fixed expenses of 100 plus variable expenses from a subitem. Variable expenses equal 60% of revenues, and require the VariableExpenses class to navigate up and over to the Revenue class.\nThis is a simple model. All values are hardcoded into the function. It is purely meant to demonstrate how the Node class helps climb the model structure.\nclass Revenue(Node[\u0026#34;NetIncome\u0026#34;]): def __call__(self, year: int): return 1000 * (1.1 ** (year - 2020)) class VariableExpenses(Node[\u0026#34;NetIncome\u0026#34;]): def __call__(self, year: int): # Find NetIncome class ni = self.find_parent(NetIncome) return ni.revenue(year) * -0.6 # 60% of revenue class Expenses(Node[\u0026#34;NetIncome\u0026#34;]): variable: \u0026#34;VariableExpenses\u0026#34; def __call__(self, year: int): # total expenses = 100 + variable expenses return -100 + self.variable(year) class NetIncome(Node[None]): revenue: \u0026#34;Revenue\u0026#34; expenses: \u0026#34;Expenses\u0026#34; def __call__(self, year: int): return self.revenue(year) + self.expenses(year) Confirm it works as expected.\nincome = NetIncome(revenue=Revenue(), expenses=Expenses(variable=VariableExpenses())) income(2025) # 544.2040000000002 Final thoughts #Protocols: Typing for parent and in find_parent isn\u0026rsquo;t limited to concrete classes. You can use protocols introduced in PEP 544 and added in Python 3.8 to define or match parents using structural subtyping.\nValidation: Node currently does not validate that its parent matches the type declared in concrete subclasses. This could lead to differences between the type hints and actual parent type. Adding validation would help avoid such issues.\n","date":"23 January 2024","permalink":"/posts/improve-model-navigation/","section":"Posts","summary":"The full example code for this post is here.","title":"Improve Model Navigation"},{"content":"","date":null,"permalink":"/","section":"jrdnh","summary":"","title":"jrdnh"},{"content":"","date":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"This post creates a model that mirrors the spreadsheet here.\nThe objective of this post is to create a financial model for an apartment building. Each line item of the operating model should accept arbitrary from_date and to_date parameters. For example, net operating income (NOI):\nnoi(from_date=date(yyyy, mm, dd), to_date=date(yyyy, mm, dd)) Sub-items, effective gross income (EGI) and operating expenses (opex) in the case of NOI, should be accessible as attributes all the way down the basic operating assumptions.\nnoi.egi(from_date=date(yyyy, mm, dd), to_date=date(yyyy, mm, dd)) noi.opex(from_date=date(yyyy, mm, dd), to_date=date(yyyy, mm, dd)) # net operating income \u0026gt; effective gross income \u0026gt; potential gross rent \u0026gt; average monthly rent per square foot noi.egi.gross_potential_rent.avg_monthly_rent_psf(from_date=date(yyyy, mm, dd), to_date=date(yyyy, mm, dd)) Finally, the model needs to be able to match the equivalent Excel model. In particular, it should be able to capture discrete period changes (to mirror an Excel model where columns represent months) even though line item functions should accept arbitrary date ranges.\nThis post discusses some of the successes and challenges in meeting these modeling objectives. It isn\u0026rsquo;t a definitive guide. It also does not step through every line of the companion files.\nCompanion file setup Model construction Model overview Line item classes Year fractions Model usage Challenges with this approach Performance and profiling details Companion file setup #The three companion files for this post are in this Gist. The files are:\nmodels.py - Pydantic-compatible wrappers relativedelta similar to the ones in this previous post, and a FixedIntervalSeries class that yields a series of dates at a fixed interval similar to the class in this previous post. utils.py - Utility functions for working with dates, iterables, and printing. companion.py - The companion script that creates and runs the model. The companion files were created with Python 3.12 and use pydantic==2.5.2 and python-dateutil==2.8.2. Other versions may work but have not been tested.\nModel construction #Model overview #The model is structured as tree of financial line items. The nodes are classes. Class attributes are annotated with their types.\nNetOperatingIncome ├── EffectiveGrossIncome │ ├── AvgVacancyRate │ │ └── vacancy_rates: list[float] │ └── GrossPotentialRent │ ├── vacancy: method │ ├── sf: int │ └── AvgMonthlyRentPSF │ ├── initial_rent_psf: float │ └── rent_growth_rate: float └── TotalExpenses ├── OperatingExpenses │ ├── units: int │ ├── initial_opex_pu: float │ └── opex_growth_rate: float ├── RealEstateTaxes │ ├── sf: int │ ├── monthly_re_tax_psf: float │ └── ret_growth_rate: float └── ReplacementReserves ├── units: int ├── annual_reserves_pu: float └── rr_growth_rate: float Each class is a callable that takes from_dt: date and to_dt: date arguments and returns the total (or in some cases average) line item value. Values are from but excluding the first date, and to and including the second date. This argument convention aligns with using consecutive (start date, end date] periods.\nLine item classes #Building line items as class instances with subitems as attributes gives the model structure. Subitems are accessible through regular dot notation. Each line item is a separate class, although OperatingExpenses, RealEstateTaxes, and ReplacementReserves are essentially the same classes with different property names. Each of these three classes model expenses that start at some value and grow stepwise at some specified, fixed interval (monthly, semi-annually). This type of periodic growth is a common pattern, so it might make sense to abstract these lines into a separate class.\nEach line item class is a subclass of FixedIntervalSeries from the models.py file. The FixedIntervalSeries class takes ref_date: date and freq: RelativeDelta constructor parameters. It has a single method periods that returns a generator yielding a series of dates with a constant offset defined by the freq property.\nclass FixedIntervalSeries(BaseModel): ref_date: date freq: RelativeDelta def periods(self): index = 0 curr_date = self.ref_date while True: yield (curr_date + self.freq * index) index += 1 It can be used as follows to create a series of evenly spaced dates.\nfrom datetime import date from models import FixedIntervalSeries, RelativeDelta series = FixedIntervalSeries(ref_date=date(2020, 1, 1), freq=relativedelta(months=1)) series_generator = series.periods() print([next(series_generator) for _ in range(3)]) # [datetime.date(2020, 1, 1), datetime.date(2020, 2, 1), datetime.date(2020, 3, 1)] Or using itertools.pairwise, create series of (start_date, end_date) tuples defining both edges of a period.\nfrom itertools import islice, pairwise list(islice(pairwise(series.periods()), 2)) # [(datetime.date(2020, 1, 1), datetime.date(2020, 2, 1)), (datetime.date(2020, 2, 1), datetime.date(2020, 3, 1))] The periods function provides the time events required to calculate discrete time intervals. Note that most of the periods functions in this model yield evenly spaced dates, but yielding the next eventful date (regardless of time interval) is the only requirement.\nLooping over periods, in combination with itertools.takewhile and year fraction functions, gives us a way to calculate values dependent on discrete intervals over arbitrary time periods. The annotated RealEstateTaxes class shows how you can calculate tax expense that increases a fixed percentage at fixed intervals. Initializing with freq=RelativeDelta(years=1) and ret_growth_rate=0.05 would step up the expense by 5.0% on the anniversary of each ref_date.\nclass RealEstateTaxes(FixedIntervalSeries): sf: int # square feet monthly_re_tax_psf: float # monthly real estate taxes per square foot ret_growth_rate: float # annual real estate tax growth rate def __call__(self, from_dt: date, to_dt: date): \u0026#34;\u0026#34;\u0026#34;Real estate taxes from but excluding `from_dt` to and including `to_dt`.\u0026#34;\u0026#34;\u0026#34; # Generator over (period index, (period start date, period end date)), # stopping when periods are past the requested date range periods = enumerate(takewhile(lambda p: p[0] \u0026lt; to_dt, pairwise(self.periods()))) # Initialize values accumulated_amt = 0 period_amt = self.monthly_re_tax_psf * self.sf * 12 # first period amount, annualized # Loop over each period and calculate the applicable tax expense for i, (per_start, per_end) in periods: # Since we want the expense for the first period to equal the initial `monthly_re_tax_psf`, # only increase the expense if i \u0026gt; 0 if i \u0026gt; 0: period_amt = period_amt * (1 + self.ret_growth_rate * YF.monthly(per_start, per_end)) # Add the expense for the portion of each period that overlaps with (from_dt, to_date) accumulated_amt += period_amt * max(YF.monthly(max(per_start, from_dt), min(per_end, to_dt)), 0) return accumulated_amt This approach uses the actual dates in each period to determine tax expense for each period. It would be less code to just increase the tax expense each period by multiplying the previous period\u0026rsquo;s expense by the growth rate:\ndef __call__(self, from_dt: date, to_dt: date): ... for (start, end) in periods: period_amt *= (1 + self.ret_growth_rate * YF.thrity360(start, end)) ... The issue with this approach is that it ties the starting value monthly_re_tax_psf to the interval duration freq. If periods do not have the same duration or you want to change the period frequency, the starting value no longer works.\nYear fractions #The YF class in utils.py holds three year fraction formulas: actual360 for an actual/360 day count convention (=YEAR(,,2) in Excel), thirty360 for a 30/360 convention (=YEAR(,,0) in Excel), and monthly (which does not have an Excel equivalent).\nThe monthly function returns one 1/12th for each whole calendar month and the pro rata portion of any stub months based on the actual number of days elapsed and the actual number of days in the month. It is similar to a 30/360 convention with a few changes.\n30/360 `YF.monthly` Whole non-calendar months\n(e.g. 2020-06-15 to 2020-07-15)\nEquals 1/12th of a year\n=YEARFRAC(\"2020-06-15\", \"2020-07-15\", 0)\n0.08333333333333333 Depends whether the stub starting month and ending month have the same number of days\n\u0026gt;\u0026gt;\u0026gt; YF.monthly(date(2020,6,15), date(2020,7,15))\n0.08198924731182795 Month end to month end periods Depends on whether the month is February or March\n=YEARFRAC(\"2020-01-31\", \"2020-02-29\", 0)\n0.0805555555555556\n=YEARFRAC(\"2020-02-29\", \"2020-03-31\", 0)\n0.0861111111111111 Always equals 1/12th of a year\n\u0026gt;\u0026gt;\u0026gt; YF.monthly(date(2020,1,31), date(2020,2,29))\n0.08333333333333333\nLast day of a month with 31 days Equals zero\n=YEARFRAC(\"2020-07-31\", \"2020-07-31\", 0)\n0.0 Equals 1/31st of a month [i.e. (1 / 31) / 12]\n\u0026gt;\u0026gt;\u0026gt; YF.monthly(date(2020,7,30), date(2020,7,31))\n0.0026881720430107525 YF.monthly allows you to create discrete, even calendar month accruals and growth rates from two dates. While not necessarily required to develop a good model, it is helpful when trying to tie back to an Excel model where users model monthly columns of data.\nModel usage #This model is fully Pydantic-compatible. You can build the model from the JSON assumptions hosted in this Gist. Assuming you are in the same directory has the companion file companion.py and have the pydantic and python-dateutil dependencies installed, you can create an instance of the model with the commands below.\nimport json import urllib.request from companion import NetOperatingIncome url = \u0026#39;https://gist.githubusercontent.com/jrdnh/377f13e0ed0e6ac975b5d36156dd27f5/raw/44bb448d60ff6aae9cc5f5d9472edf9e0c0b85d3/noi.json\u0026#39; with urllib.request.urlopen(url) as response: noi = NetOperatingIncome.model_validate_json(response.read()) Now you have a full model of financial projetions for this apartment building!\nTry getting net operating income over the first year.\nfrom datetime import date noi(date(2019, 12, 31), date(2020, 12, 31)) # 4667040.0 Get the average monthly rent per square foot between two random dates (for example, 2 June 2021 to 27 September 2023).\nnoi.effective_gross_income.gross_potential_rent.avg_monthly_rent_psf(date(2021, 6, 2), date(2023, 9, 27)) # 3.292631202107785 Line items are FixedIntervalSeries instances. If we want to display the full pro forma for all line items on a monthly basis, we can recursively iterate over each attribute and try calling it with a series of monthly periods. The helper field_values creates a nested dictionary of line items with their values, and flatten flattens the dictionary.\nfrom itertools import pairwise from utils import field_values, flatten from models import RelativeDelta ten_years_monthly = list(pairwise((date(2019, 12, 31) + RelativeDelta(months=1) * i for i in range(121)))) pro_forma = flatten(field_values(noi, ten_years_monthly)) print(json.dumps(pro_forma, indent=2)) # long json output Pandas displays tables nicely. It is also easy to copy DataFrames to the clipboard or write directly to a .xlsx file. Make sure you have pandas installed before running the following code.\nimport pandas as pd df = pd.DataFrame(pro_forma, index=[p[1] for p in ten_years_monthly]).T # df.to_clipboard() # df.to_excel(\u0026#39;pro_forma.xlsx\u0026#39;) print(df) # 2020-01-31 2020-02-29 2020-03-31 ... 2029-10-31 2029-11-30 2029-12-31 # .effective_gross_income.avg_vacancy_rate 0.10 0.10 0.10 ... 0.050000 0.050000 0.050000 # .effective_gross_income.gross_potential_rent.av... 3.16 3.16 3.16 ... 3.776493 3.776493 3.776493 # .effective_gross_income.gross_potential_rent 568800.00 568800.00 568800.00 ... 679768.653032 679768.653032 679768.653032 # .effective_gross_income 511920.00 511920.00 511920.00 ... 645780.220381 645780.220381 645780.220381 # .total_expenses.operating_expenses -63250.00 -63250.00 -63250.00 ... -75589.604965 -75589.604965 -75589.604965 # .total_expenses.real_estate_taxes -54000.00 -54000.00 -54000.00 ... -64534.998706 -64534.998706 -64534.998706 # .total_expenses.replacement_reserves -5750.00 -5750.00 -5750.00 ... -6871.782270 -6871.782270 -6871.782270 # .total_expenses -123000.00 -123000.00 -123000.00 ... -146996.385941 -146996.385941 -146996.385941 # . 388920.00 388920.00 388920.00 ... 498783.834440 498783.834440 498783.834440 # [9 rows x 120 columns] Challenges with this approach # Sibling dependencies: You might have noticed that vacancy is modeled using a method of EffectiveGrossIncome instead of as a custom class. That\u0026rsquo;s because it relies on gross potential rent which is a sibling in the hierarchy (vacany expense = gross potential rent * vacancy rate). For more complicated leasing arrangements such as net leases, reimbursement lines in the revenue section rely on lines buried all they way down in the expense section of the statement. Using dependencies down the tree work well, but dependencies up and across the tree are difficult to navigate. Assumption references: If you look at the serialized assumptions for this model you\u0026rsquo;ll see a lot of the same values. In this case, all the ref_dates are the same analysis start date of December 12, 2019. If you wanted to change the analysis start date, you\u0026rsquo;d have to update the assumption in many places. Performance: The periods date generators are inefficient. Calculating a full 10-year schedule of monthly values for all line items takes more than a third of second. periods is called almost 25,000 times. Practical models generally have mid to high hundreds of line items which implies this model is too slow to be useful. periods is a good candidate for caching though, both within subclasses and potentially across subclasses, since the same arguments are called repeatedly and generator outputs are small. Performance and profiling details #Time duration to create a full 10-year schedule of monthly values.\nfrom time import perf_counter def duration(): start = perf_counter() field_values(noi, ten_years_monthly) print((perf_counter() - start) * 1000, \u0026#39;ms\u0026#39;) duration() # 370.89961499441415 ms Key profiling results.\nimport cProfile import pstats profiler = cProfile.Profile() profiler.enable() field_values(noi, ten_years_monthly) profiler.disable() stats = pstats.Stats(profiler).sort_stats(\u0026#39;tottime\u0026#39;) stats.print_stats(10) # 1129336 function calls (1129297 primitive calls) in 0.870 seconds # Ordered by: internal time # List reduced from 53 to 10 due to restriction \u0026lt;10\u0026gt; # ncalls tottime percall cumtime percall filename:lineno(function) # 21358 0.130 0.000 0.265 0.000 /.../dateutil/relativedelta.py:317(__add__) # 21358 0.084 0.000 0.305 0.000 /.../dateutil/relativedelta.py:495(__mul__) # 62662 0.083 0.000 0.152 0.000 /.../python3.12/calendar.py:154(weekday) # 21358 0.063 0.000 0.222 0.000 /.../dateutil/relativedelta.py:105(__init__) # 42716 0.056 0.000 0.102 0.000 {built-in method builtins.any} # 20652 0.043 0.000 0.161 0.000 /.../utils.py:50(monthly) # 62662 0.034 0.000 0.187 0.000 /.../calendar.py:161(monthrange) # 62662 0.033 0.000 0.054 0.000 /.../python3.12/enum.py:709(__call__) # 21358 0.030 0.000 0.049 0.000 /.../dateutil/relativedelta.py:231(_fix) # 24598 0.030 0.000 0.613 0.000 /.../models.py:105(periods) Most of the time is spent creating, multiplying, and adding dateutil.relativedelta.relativedelta objects inside the body of FixedIntervalSeries.periods. FixedIntervalSeries.periods is called almost 25,000 times (see last line or results table).\n","date":"15 January 2024","permalink":"/posts/create-a-multifamily-financial-model-in-python/","section":"Posts","summary":"This post creates a model that mirrors the spreadsheet here.","title":"Create a Multifamily Financial Model in Python"},{"content":"Spreadsheets remain undefeated for financial modeling1. There are a number of generic tools (e.g. Anaplan) and industry specific tools (e.g. Argus), but they are best used in portfolio management-type scenarios where scale, structure, or auditability are valued over flexibility. Even then, many companies still use Excel for these activities. In underwriting and valuation situtaions, Excel dominates.\nExcel is fantastic, but there a times I wish it had the full power of programming environment. Excel struggles with certain data transformations (e.g. unique attribute counts with filtering and grouping) and importing/exporting data to/from external sources.\nThis article plays with an approach to fundamental modeling in Python. I haven\u0026rsquo;t seen other robust approaches, but I\u0026rsquo;d be interested in hearing about them!2\nThe setup\u0026ndash;a time-based loop Refactor to make it a bit more generic Prettier outputs Saving and loading models The setup\u0026ndash;a time-based loop #I\u0026rsquo;d like to be able to do something like this where from_date and to_date can be any two arbitrary dates. In Excel, you\u0026rsquo;re generally limited to calculating values at whatever frequency columns represent (e.g. monthly, quarterly, annually). Sometimes it\u0026rsquo;s helpful to have intra-period granularity to capture specific events or accruals. Since we\u0026rsquo;re not restricted to a graphical grid, there\u0026rsquo;s no reason to limit ourselves to a fixed intervals.\nobj.revenue(from_date, to_date) # sum of revenue between dates Cash flows are frequently are modeled by growing some starting value at a periodically compounding rate. In the most basic cases, such series can be modeled as a geometric series. In most real-world scenarios, growth is more complex and needs to be modeled iteratively.\nA recursive approach would also generally work. Unfortnately, when I\u0026rsquo;ve tried to create even medium-sized models using recursive calls, the call stack blows up and I hit the recursion limit (default of 1,000 frames typically).\nThe cash flow function needs to know the periods over which to compound. We can start by creating a generator function that drives each period of the model.\nfrom datetime import date from pydantic import BaseModel # from relativedelta import RelativeDelta, see note below class Statement(BaseModel): ref_date: date freq: RelativeDelta def periods(self): per_start, per_end = self.ref_date, self.ref_date + self.freq while True: yield (per_start, per_end) per_start, per_end = per_end, per_end + self.freq statement = Statement(ref_date=date(2020, 12, 31), freq=RelativeDelta(months=1, day=31)) s = statement.periods() [next(s) for _ in range(3)] # [(datetime.date(2020, 12, 31), datetime.date(2021, 1, 31)), ...] RelativeDelta is a Pydantic-compatible version of dateutil.relativedelta.relativedelta. Read about it in this previous post, or copy the code from this Gist.\nNow that we have a function that yields the compounding periods, we can add a function that calculates the appropriate flows. For example using the revenue line item:\nclass Statement(BaseModel): ref_date: date freq: RelativeDelta ref_revenue: float # reference revenue rev_growth: float # annual revenue growth rate def periods(self): per_start, per_end = self.ref_date, self.ref_date + self.freq while True: yield (per_start, per_end) per_start, per_end = per_end, per_end + self.freq def revenue(self, from_date: date, to_date: date): accum_revenue, period_rev = 0, self.ref_revenue for per_start, per_end in self.periods(): # if the period is past the end date, break if per_start \u0026gt;= to_date: break # otherwise add pro rata period revenue to accumulated revenue period_length = (per_end - per_start).days period_rev = period_rev * (1 + self.rev_growth * period_length / 365) accum_revenue += ( period_rev * max((min(per_end, to_date) - max(per_start, from_date)).days, 0) / period_length ) return accum_revenue First, add attributes for the starting amount of revenue and annual growth rate. Note that in this case the ref_revenue is the amount or revenue for the period ending on ref_date. In other words, revenue for the first period of the model equals ref_revenue * (1 + rev_growth * year_frac).\nThe revenue function initializes a variable to hold accumulated revenue between the two input dates and a variable to hold revenue for the current period. For each period, the function checks if we\u0026rsquo;ve gone past the to_date and breaks if so. Otherwise, it calculates revenue for the current period and adds it to the accumulated revenue (for any portion of the current period that overlaps with the input dates).\nstatement = Statement( ref_date=date(2019, 12, 31), freq=RelativeDelta(months=1, day=31), # month end frequency ref_revenue=1000, rev_growth=0.1, ) # revenue between two random dates statement.revenue(from_date=date(2020, 2, 3), to_date=date(2020, 7, 11)) # 5439.378625854814 s = statement.periods() [f\u0026#34;{statement.revenue(*next(s)):.2f}\u0026#34; for _ in range(5)] # [\u0026#39;1008.49\u0026#39;, \u0026#39;1016.51\u0026#39;, \u0026#39;1025.14\u0026#39;, \u0026#39;1033.56\u0026#39;, \u0026#39;1042.34\u0026#39;] Refactor to make it a bit more generic #The operating statement has a revenue line item. Let\u0026rsquo;s add a line for fixed costs next. We could model fixed costs using the same method: an initial value that grows at an annual rate, compounding over each period. The code is almost exactly the same as revenue. This is a common pattern, so let\u0026rsquo;s pull it out into a separate function.\nfrom typing import Callable, Iterable # pull out the accumulation code from `revenue` into a separate function def accumulate( from_date: date, to_date: date, start_amt: float, growth_rate: float, year_frac: Callable[[date, date], float], periods: Iterable[tuple[date, date]], ): accum_amt = 0 period_amt = start_amt for per_start, per_end in periods: if per_start \u0026gt;= to_date: break period_yf = year_frac(per_start, per_end) period_amt = period_amt * (1 + growth_rate * period_yf) accum_amt += ( period_amt * max(year_frac(max(per_start, from_date), min(per_end, to_date)), 0) / period_yf ) return accum_amt def actual365(from_date: date, to_date: date): \u0026#34;\u0026#34;\u0026#34;Year fraction on an actual/365 basis\u0026#34;\u0026#34;\u0026#34; return (to_date - from_date).days / 365 class Statement(BaseModel): ref_date: date freq: RelativeDelta ref_revenue: float # reference revenue rev_growth: float # annual revenue growth rate ref_fixed_costs: float # reference fixed costs fixed_costs_growth: float # annual fixed costs growth rate def periods(self): per_start, per_end = self.ref_date, self.ref_date + self.freq while True: yield (per_start, per_end) per_start, per_end = per_end, per_end + self.freq def revenue(self, from_date: date, to_date: date): return accumulate( from_date, to_date, self.ref_revenue, self.rev_growth, actual365, self.periods(), ) def fixed_costs(self, from_date: date, to_date: date): return accumulate( from_date, to_date, self.ref_fixed_costs, self.fixed_costs_growth, actual365, self.periods(), ) This is still a lot of code, but at least it\u0026rsquo;s easier to add new line items to the operating statement. However, this model only works with revenue and fixed_costs compound at the same frequency. Additionally, revenue and fixed_costs are not re-usable if we wanted to create another operating model.\nThere are two main concepts here.\nA driver function that yields each successive period in the series. In this case, each period is at a fixed interval, but you could easily imagine other scenarios. A growth function that accumulates the flow over the appropriate period. In this case, accumulation is based i) on a constant annual growth rate, and ii) the proportionate amount of any overlapping periods. In other cases, the growth rate might not be constant or the accumulation function might not include partial periods (e.g. cash accounting for payments received on the final day of the period). Let\u0026rsquo;s create driver and growth classes that we can use for revenue and fixed_costs.\nclass FixedPeriodSeries(BaseModel): ref_date: date freq: RelativeDelta def periods(self): per_start, per_end = self.ref_date, self.ref_date + self.freq while True: yield (per_start, per_end) per_start, per_end = per_end, per_end + self.freq class GrowingSeries(FixedPeriodSeries): ref_amount: float growth_rate: float def __call__(self, from_date: date, to_date: date): return accumulate( from_date, to_date, self.ref_amount, self.growth_rate, actual365, self.periods(), ) class OperatingStatement(BaseModel): revenue: GrowingSeries fixed_costs: GrowingSeries # income def __call__(self, from_date: date, to_date: date): return self.revenue(from_date, to_date) - self.fixed_costs(from_date, to_date) FixedPeriodSeries is the period driver function. It yield periods based on a fixed time interval. The GrowingSeries class is the growth function. It determines amount based on fixed annual growth and includes partial periods. They are tied together to reflect the chosen modeling structure.\nThis is the extent of the line item modeling we are going to do here. If we were building out a full three statement model in practice, we\u0026rsquo;d probably want to think through the common patterns and create a more robust set of helper classes. For example, we might have line items with contingent events (e.g. an earnout paid when certain performance metrics are achieved) where driver periods are dynamic. Or you might want to different short- and long-term growth rates. Having the growth function inherit from the driver function may not make sense in a larger model either.\nstart_date = date(2019, 12, 31) statement = OperatingStatement( revenue=GrowingSeries( ref_date=start_date, freq=RelativeDelta(months=1, day=31), ref_amount=1000, growth_rate=0.1, ), fixed_costs=GrowingSeries( ref_date=start_date, freq=RelativeDelta(months=3, day=31), ref_amount=500, growth_rate=0.05, ), ) statement(start_date, date(2020, 12, 31)) # income for the next year # 10607.424004381839 statement.revenue(start_date, date(2020, 12, 31)) # revenue for the next year # 12670.746995800495 This setup allows us to nicely access line items (and sub-line items if we had anything) using regular dot notation since each line item is a regular attribute.\nWe can confirm that there aren\u0026rsquo;t any recursion issues as well. We can recreate the same model with daily revenue compounding and calculate total income over the next 10,000 days (December 31, 2019 to May 18, 2047).\ndaily_statement = OperatingStatement( revenue=GrowingSeries( ref_date=start_date, freq=RelativeDelta(days=1), # daily frequency ref_amount=1000, growth_rate=0.1, ), fixed_costs=GrowingSeries( ref_date=start_date, freq=RelativeDelta(months=3, day=31), ref_amount=500, growth_rate=0.05, ), ) end_date = start_date + RelativeDelta(days=10_000) daily_statement.revenue(start_date, end_date) # 52855286.28641588 Prettier outputs #One of the great things about Excel is that you can see the data easily. Every cell is visible. It would be really nice to present the outputs in a familiar format here: periods as columns and line items as rows.\nEach node in our model is a callable Pydantic BaseModel. We can create some helper functions to iterate over each model field and recursively call the node for each period. This will give us a nested dictionary of values representing our pro forma model.\ndef field_values(series: BaseModel, periods): \u0026#34;\u0026#34;\u0026#34;Recursively get values of all FixedPeriodSeries fields\u0026#34;\u0026#34;\u0026#34; values = {} try: for field in series.model_fields: try: values[field] = field_values(getattr(series, field), periods) except TypeError: pass except AttributeError: pass values.update({\u0026#34;\u0026#34;: [series(*period) for period in periods]}) return values Let\u0026rsquo;s print out the first year of the model monthly.\nfrom itertools import pairwise import json periods = list(pairwise([start_date + RelativeDelta(months=1, day=31) * i for i in range(13)])) pro_forma = field_values(statement, periods) print(json.dumps(pro_forma, indent=2)) # { # \u0026#34;revenue\u0026#34;: { # \u0026#34;\u0026#34;: [1008.4931506849315, 1016.5058359917433, 1025.1391732289333, ...] # }, # \u0026#34;fixed_costs\u0026#34;: { # \u0026#34;\u0026#34;: [172.4529580009032, 161.32696071052234, 172.4529580009032, ...] # }, # \u0026#34;\u0026#34;: [836.0401926840283, 855.1788752812209, 852.68621522803, ...] # } We can improve the nested dictionary by flattening it out.\ndef _flatten_gen(d: dict, prefix=\u0026#34;.\u0026#34;): for key, value in d.items(): if isinstance(value, dict): yield from _flatten_gen(value, prefix + key) else: yield prefix + key, value def flatten(d: dict): \u0026#34;\u0026#34;\u0026#34;Flatten a nested dictionary\u0026#34;\u0026#34;\u0026#34; return dict(_flatten_gen(d)) Each line item and subline item is delimited by a period.\nflat_pro_forma = flatten(pro_forma) print(json.dumps(flat_pro_forma, indent=2)) # { # \u0026#34;.revenue\u0026#34;: [1008.4931506849315, 1016.5058359917433, 1025.1391732289333, ...] # \u0026#34;.fixed_costs\u0026#34;: [172.4529580009032, 161.32696071052234, 172.4529580009032, ...] # \u0026#34;.\u0026#34;: [836.0401926840283, 855.1788752812209, 852.68621522803, ...] # } Incorporating these pretty outputs into the base objects would be helfpul, but I\u0026rsquo;m not going to spend the time to do that here.\nThe pretty pro forma dictionary makes exporting results to pandas easy. From there, you could easily copy to the clipboard or save to Excel.\nimport pandas as pd df = pd.DataFrame(flat_pro_forma, index=[p[1] for p in periods]).T print(df) # 2020-01-31 2020-02-29 2020-03-31 ... 2020-10-31 2020-11-30 2020-12-31 # .revenue 1008.493151 1016.505836 1025.139173 ... 1086.774661 1095.707056 1105.013061 # .fixed_costs 172.452958 161.326961 172.452958 ... 177.085398 171.372966 177.085398 # . 836.040193 855.178875 852.686215 ... 909.689263 924.334090 927.927663 # [3 rows x 12 columns] # df.to_clipboard() # df.to_excel(\u0026#39;pro_forma.xlsx\u0026#39;) Saving and loading models #We haven\u0026rsquo;t exploited the power of Pydantic yet. In fact, nothing so far really requires Pydantic. However, it gives us a nice way to separate our data from our models. We can save our data somewhere else, load it into our model when we want to evaluate some metric, and then serialize it back out to some storage with any updates. It also gives third-parties a way to update the data independently of the model.\n# write the model to a file with open(\u0026#34;my_company.json\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(statement.model_dump_json()) # mimic some external system that updates the data for higher expected revenue growth with open(\u0026#34;my_company.json\u0026#34;, \u0026#34;r\u0026#34;) as f: data = json.load(f) data[\u0026#34;revenue\u0026#34;][\u0026#34;growth_rate\u0026#34;] = 0.15 with open(\u0026#34;my_company.json\u0026#34;, \u0026#34;w\u0026#34;) as f: json.dump(data, f) # create new model from updateddata with open(\u0026#34;my_company.json\u0026#34;, \u0026#34;r\u0026#34;) as f: new_statement = OperatingStatement.model_validate_json(f.read()) assert new_statement.revenue.growth_rate == 0.15 Difficulty interacting with external data sources is one of the biggest challenges with Excel. Pydantic makes it significantly easier to deal with serializing to and validating data from external sources. In some cases you may need to create custom serializers or validators, but that it\u0026rsquo;s much more flexible than Power Query or VBA.\nFinal thoughts #This is a very basic example. It\u0026rsquo;s not at all clear that it scales well to larger or more complex models. The hierarchical structure around the financial statements works well, but without better ways to explore the structure visually it would be difficult to figure out how a complex model is constructed (not that stepping through someone else\u0026rsquo;s workbook is any easier). Ripe for improvements, and perhaps a different approach altogether.\nThe full code for this post can be found here.\nSpecifically I mean Excel. Sheets and Numbers are not serious contenders in finance.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI\u0026rsquo;m sure they\u0026rsquo;re out there. I just haven\u0026rsquo;t found them yet.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"7 January 2024","permalink":"/posts/basic-financial-modeling-with-python/","section":"Posts","summary":"Spreadsheets remain undefeated for financial modeling1.","title":"Basic Financial Modeling With Python"},{"content":"Each code snippet should run as a standalone example (based on Python 3.12).\nThe standard library caching decorator functools.lru_cache has known limitations when used with instance methods. In particular, the cache is a property of the class and holds references to each function argument. Since instances themselves are the first argument to instance methods (self), the class will end up storing a reference to each instance. This can lead to memory leaks where the instance\u0026rsquo;s reference count never reaches zero, so it\u0026rsquo;s never removed by the garbage collector.\nfrom functools import lru_cache import gc class Foo: @lru_cache def bar(self): print(\u0026#39;bar called\u0026#39;) return \u0026#39;bar\u0026#39; def __del__(self): print(f\u0026#39;deleting Foo instance {self}\u0026#39;) foo = Foo() foo.bar() # bar called # \u0026#39;bar\u0026#39; # delete foo del foo gc.collect() # 0, nothing collected The simplest solution is use @staticmethods. The instance object is not passed as the first argument to static methods, so the cache will not hold a reference to the instance. However, this is not always possible.\nThere has been some discussion among core developers about addressing this issue in the standard library. In the meantime, there are a few workarounds.\nUse weakref to store a reference to the instance Make lru_cache an instance attribute Make the cache an instance attribute Use weakref to store a reference to the instance #The first workaround is to use weakref to store a reference to the instance (i.e. the self argument). In fact, caching is a use case explicitly cited by the weakref module documentation. A solution using weakref might look like this:\nimport functools import weakref def weak_cache(func): cache = {} @functools.wraps(func) def wrapper(self, *args, **kwargs): # wrap `self` with weakref to avoid memory leaks weak_self = weakref.ref(self) key = (weak_self, args, tuple(sorted(kwargs.items()))) if key in cache: return cache[key] result = func(self, *args, **kwargs) cache[key] = result return result return wrapper Confirm that it works:\nclass Foo: @weak_cache def bar(self): print(\u0026#39;bar called\u0026#39;) return \u0026#39;bar\u0026#39; def __del__(self): print(f\u0026#39;deleting Foo instance {self}\u0026#39;) foo = Foo() foo.bar() # bar called # \u0026#39;bar\u0026#39; del foo # deleting Foo instance \u0026lt;__main__.Foo object at 0x10d2e1a30\u0026gt; This approach works fine in many cases. However, not all objects can be weakly referenced. For example, in CPython built-ins such as tuple and int do not support weak references even when subclassed. Additionally, types with __slots__ cannot by weakly referenced unless they have a __weakref__ slot.\nMake lru_cache an instance attribute #Another solution is to make the cached function a property of the instance. Since the class no longer holds a reference to the instance through the cache, it does not keep the instance alive.\nclass Foo: def __init__(self) -\u0026gt; None: self.bar = lru_cache()(self._bar) def _bar(self): return \u0026#39;bar\u0026#39; def __del__(self): print(f\u0026#39;deleting Foo instance {self}\u0026#39;) foo = Foo() foo.__dict__ # show that the `lru_cache` warpper is an instance attribute # {\u0026#39;bar\u0026#39;: \u0026lt;functools._lru_cache_wrapper object at 0x10a614bf0\u0026gt;} foo.bar() # \u0026#39;bar\u0026#39; foo = None gc.collect() # deleting Foo instance \u0026lt;__main__.Foo object at 0x103298b30\u0026gt; # 9 The primary drawback with this approach is that it requires additional work to set the cached function as an attribute in the __init__ method.\nMake the cache and instance attribute #A third solution is to store just the cache as a property of the instance. This is similar to the previous solution, but just the cache is set as the instance attribute. The function wrapper—which we will have to implement—remains an attribute of the class. This avoids the need to set the cached function as an attribute in __init__. It can be used simply as a decorator on instance methods.\nfrom functools import wraps def caching_decorator(func): cachename = f\u0026#34;_cached_{func.__qualname__}_\u0026#34; @wraps(func) def wrapper(self, *args): # try to get the cache from the object cachedict = getattr(self, cachename, None) # if the object doen\u0026#39;t have a cache, try to create and add one if cachedict is None: cachedict = {} setattr(self, cachename, cachedict) # try to return a cached value, # or if it doesn\u0026#39;t exist, create it, cache it, and return it try: return cachedict[args] except KeyError: pass value = func(self, *args) cachedict[args] = value return value return wrapper The decorator wraps instance methods with a function that checks the instance object for a matching cache attribute name. If the cache attribute exists, it is assumed to be a dictionary mapping function arguments to results. If the attribute does not exist, it is created. The function is then called and the result is stored in the dictionary before it is returned.\nclass Foo: @caching_decorator def bar(self, num: int): print(f\u0026#39;bar called with {num}\u0026#39;) return \u0026#39;bar\u0026#39; def __del__(self): print(f\u0026#39;deleting Foo instance {self}\u0026#39;) foo = Foo() foo.bar(1) # bar called with 1 # \u0026#39;bar\u0026#39; foo.bar(1) # cached result is returned # \u0026#39;bar\u0026#39; foo.__dict__ # the cache is stored as an instance attribute # {\u0026#39;_cached_Foo.bar_\u0026#39;: {(1,): \u0026#39;bar\u0026#39;}} foo = None # deleting Foo instance \u0026lt;__main__.Foo object at 0x108dc6b70\u0026gt; This approach requires work to implement. For instance, this version only works with positional arguments. There is also a risk of name collisions and name pollution. It is flexible and exposes the cache to the user which is helpful for building the generator cache in the next section.\nDealing with mutability #Mutability and hash-ability are a pervasive issues with caching in Python. The functools.lru_cache decorator, as well as all the other approaches discussed here, store results in a dictionary. Dictionary keys must be hashable. The function arguments are used as keys, which means they must be hashable as well. This is not a problem for most built-in, non-collection types, but it can be an issue for custom classes. Since the first argument to instance methods is the instance itself, the instance must be hashable. This Lyft Engineering blog post has a good discussion of the issue.\nThe default behavior of hash(self) for custom classes is based on the object\u0026rsquo;s ID. Object ID\u0026rsquo;s are guaranteed to remain the same over the life of the object. As long as objects are immutable, this is fine. However, mutable objects with methods that access object properties can lead to unexpected behavior. Specifically, if a property that the method relies on is changed, the incorrect cached value will continue to be used because the default hash value will not change.\nIf you can\u0026rsquo;t make objects (faux) immutable, there really isn\u0026rsquo;t a great solution. One option is to use a custom __hash__ method that returns a hash based on the object\u0026rsquo;s properties. This approach has the desirable effect of invalidating the cache whenever a property is changed. However, is approach is generally not a good idea because it can lead to unexpected behavior in other places the object is used in a dict or set. These issues tend to be hard to debug. For example, if the object is used as a dictionary key elsewhere then changing the object\u0026rsquo;s properties will change the hash value and the item will no longer be retrievable from the dictionary. Additionally, the cache will be invalidated if any property is changed, even if the method does not rely on that property.\nA similar option would be to create the cache\u0026rsquo;s key value inside the wrapper based on the object\u0026rsquo;s self.__dict__ key-value pairs (e.g. hash(k, v for k, v in self.__dict__.items())). This would avoid causing issues in other placess that __hash__ is used. However, if the method relies on mutable properties, the cache could still return the incorrect value. It also continues to invalidate the cache whenever any property is changed, even if such property isn\u0026rsquo;t used by the method.\nConclusion: Mutability is an issue with caching in Python. If you can\u0026rsquo;t protect against properties that the method relies on from changing, consider using a different approach.\nUsing Tee to cache generator results #The goal is to return a generator method that caches its results as next is called. So, if two generators are created from the method, they will share the same cache. The generator will only run once for each element of the series. This is useful for generators that are expensive to compute and are used multiple times.\nThis approach is inspired by this Stack Overflow answer. It uses itertools.tee to create two copies of the generator. tee takes an iterable and returns multiple independent iterators that pull from the same source. The documentation has a full explanation with example implementation.\nHere, tee is used to create two iterators with the same source generator. One iterator is returned to the user, and the other copy is stored in the cache. The next time the generator is called, the cached copy is used to create two new copies and the process repeats.\nfrom itertools import tee from types import GeneratorType # Get the class returned by `itertools.tee` so we can check against it later Tee = tee([], 1)[0].__class__ def memoized(f): cache={} def ret(*args): # check whether the generator has been called before with same arguments if args not in cache: # if not, call the generator function cache[args]=f(*args) # check whether the result is a generator (generator method has not been called before) # or a Tee (generator method has been called before). # this should be `True` unless the decorated method doesn\u0026#39;t return a generator (e.g. regular function) if isinstance(cache[args], (GeneratorType, Tee)): # create two new iterator copies, store one and return one cache[args], r = tee(cache[args]) return r return cache[args] return ret Using it with the fibonacci sequence shows that the print function is only called once for each element in the sequence.\n@memoized def fibonator(): a, b = 0, 1 while True: print(f\u0026#39;yielding {a}\u0026#39;) yield a a, b = b, a + b fib1 = fibonator() next(fib1) # will print \u0026#34;yielding\u0026#34; # yielding 0 # 0 fib2 = fibonator() next(fib2) # will not print \u0026#34;yielding\u0026#34;, uses cached value # 0 Copying iterables with tee trades memory for speed. Generators are often used precisely because they don\u0026rsquo;t require storing the entire series in memory. This approach conflicts with that. There are other scenarios where generators are useful, for example where the series has an indefinite length or can\u0026rsquo;t be computed ahead of time, that the tradeoff makes sense.\nPutting it all together #The following class calculates periodic revenue growing at a constant annual rate. The class has two methods: periods which returns a generator of (start, end) tuples, and amount which calculates revenue iteratively for the given period. Growth compounds each period based on the actual number of days and a 360 day year. This type of formula is common in financial modeling.\nfrom dataclasses import dataclass from datetime import date from dateutil.relativedelta import relativedelta @dataclass class Operations: start_date: date freq: relativedelta initial_rev: float growth_rate: float def periods(self): \u0026#34;\u0026#34;\u0026#34;Revenue growth periods\u0026#34;\u0026#34;\u0026#34; curr = (self.start_date, self.start_date + self.freq) while True: yield curr curr = (curr[1], curr[1] + self.freq) def amount(self, period_end: date): \u0026#34;\u0026#34;\u0026#34;Revenue for period\u0026#34;\u0026#34;\u0026#34; revenue = self.initial_rev for start, end in self.periods(): if period_end \u0026lt;= start: return revenue revenue *= 1 + self.growth_rate * (end - start).days / 360 rev = Operations(start_date=date(2020, 1, 1), freq=relativedelta(months=1), initial_rev=1000.0, growth_rate=0.1) rev.amount(date(2021, 1, 1)) # 1106.5402134963185 Each time amount is called, it iterates through a new generator returned by periods. This is ineffecient since the same time periods are used each time. Printing 10 years of monthly revenue requires iterating through the while loop 7,260 times. More generally, it requires n * (n + 1) / 2 iterations where n is the number of periods.\nfrom timeit import timeit def revenue_series(): return [rev.amount(dt) for dt in (date(2020,1,1) + relativedelta(months=i) for i in range(120))] count = 100 time = timeit(revenue_series, number=count) print(f\u0026#39;{time / count * 1000:.2f} ms per iteration\u0026#39;) # 41.06 ms per iteration In a larger operating model with hundreds of line items instead of just revenue, this can add up to a significant amount of time. We can avoid this by caching the result each time a value is calculated in periods.\nThe following wrapper combines the instance method decorator with the generator caching decorator.\nfrom itertools import tee from types import GeneratorType from functools import wraps Tee = tee([], 1)[0].__class__ def cached_generator(func): cachename = f\u0026#34;_cached_{func.__qualname__}_\u0026#34; @wraps(func) def wrapper(self, *args): # try to get the cache from the object, or create if doesn\u0026#39;t exist cache = getattr(self, cachename, None) if cache is None: cache = {} setattr(self, cachename, cache) # return tee\u0026#39;d generator if args not in cache: cache[args]=func(self, *args) if isinstance(cache[args], (GeneratorType, Tee)): cache[args], r = tee(cache[args]) return r return cache[args] return wrapper We can use it in the Operations class to make amount significantly faster. In this example, more than 10x faster.\n@dataclass class Operations: start_date: date freq: relativedelta initial_rev: float growth_rate: float @cached_generator def periods(self): \u0026#34;\u0026#34;\u0026#34;Revenue growth periods\u0026#34;\u0026#34;\u0026#34; curr = (self.start_date, self.start_date + self.freq) while True: yield curr curr = (curr[1], curr[1] + self.freq) def amount(self, period_end: date): \u0026#34;\u0026#34;\u0026#34;Revenue for period\u0026#34;\u0026#34;\u0026#34; revenue = self.initial_rev for start, end in self.periods(): if period_end \u0026lt;= start: return revenue revenue *= (1 + self.growth_rate * (end - start).days / 360) rev = Operations(start_date=date(2020, 1, 1), freq=relativedelta(months=1), initial_rev=1000.0, growth_rate=0.1) def revenue_series(): return [rev.amount(dt) for dt in (date(2020,1,1) + relativedelta(months=i) for i in range(120))] count = 100 time = timeit(revenue_series, number=count) print(f\u0026#39;{time / count * 1000:.2f} ms per iteration\u0026#39;) # 3.05 ms per iteration This example highlights a scenario where caching generator methods might be helpful. There is an indefinite number of periods, and many generators are created returning the same values. The generator results are not large, so they can be stored in memory easily. start_date and freq can both be changed, so additional care is needed to ensure the cache remains valid (e.g. protecting with @dataclass(frozen=True) to make the attributes harder to change).\nFinal thoughts # Don\u0026rsquo;t use functools.lru_cache with instance methods. It can lead to memory leaks. If you can\u0026rsquo;t use @staticmethod, consider using weakref or storing the cache as an instance property. Use itertools.tee to cache generator results (or consider calculating values ahead of time if possible). Mutability is an issue with caching in Python. If you can\u0026rsquo;t protect properties that the method relies on from changing, consider using a different approach. ","date":"30 December 2023","permalink":"/posts/caching-generator-methods-in-python/","section":"Posts","summary":"Each code snippet should run as a standalone example (based on Python 3.","title":"Caching Generator Methods in Python"},{"content":"Validating and structuring data models with Pydantic is a breeze, until you need to use incompatible third-party data types. This post walks through the process of making the dateutil.relativedelta.relativedelta type from the widely-used python-dateutil package fully Pydantic-compatible, including JSON schemas.\nThere are several straight-forward approaches if you only need to validate or serialize custom types. It gets tricker if you also want schemas. The process here follows the principles from the Pydantic documentation:\nCreate a Pydantic model that represents the third-party type Modify the model\u0026rsquo;s validation function to return an instance of the third-party type Add metadata to the third-party type that tells Pydantic how to validate, serialize, and generate schemas for it Exploring relativedelta #relativedelta is a date offset type. It\u0026rsquo;s similar to the datetime.timedelta type, but it includes other relative offsets such as years and months as well absolute offests such as the year or the calendar day of the month. Relative offsets end with an \u0026ldquo;s\u0026rdquo; (e.g. month=1 offsets to January, months=1 increments by one month).\nThe internal state of a relativedelta object is a dictionary with the following keys:\nimport json from dateutil.relativedelta import relativedelta print(json.dumps(relativedelta().__dict__, indent=2)) # { # \u0026#34;years\u0026#34;: 0, # \u0026#34;months\u0026#34;: 0, # \u0026#34;days\u0026#34;: 0, # \u0026#34;leapdays\u0026#34;: 0, # \u0026#34;hours\u0026#34;: 0, # \u0026#34;minutes\u0026#34;: 0, # \u0026#34;seconds\u0026#34;: 0, # \u0026#34;microseconds\u0026#34;: 0, # \u0026#34;year\u0026#34;: null, # \u0026#34;month\u0026#34;: null, # \u0026#34;day\u0026#34;: null, # \u0026#34;hour\u0026#34;: null, # \u0026#34;minute\u0026#34;: null, # \u0026#34;second\u0026#34;: null, # \u0026#34;microsecond\u0026#34;: null, # \u0026#34;weekday\u0026#34;: null, # \u0026#34;_has_time\u0026#34;: 0 # } We can ignore _has_time which is set at initialization. All of the other attributes are (optional) ints except for weekday which is a dateutil._common.weekday object. In order to make relativedelta Pydantic compatible, we should also make weekday Pydantic compatible.\nMake weekday Pydantic compatible #1. Create a Pydantic model for weekday #weekday uses slots and has the following state:\nfrom dateutil._common import weekday weekday(0).__slots__ # [\u0026#39;weekday\u0026#39;, \u0026#39;n\u0026#39;] The first step is to create a Pydantic model that represents weekday:\nfrom pydantic import BaseModel, Field class WeekdayAnnotations(BaseModel): weekday: int = Field(ge=0, le=6) n: int | None = None The weekday attribute is the day of the week, so it must be bounded by 0 and 6. n is the offset for number of weeks (e.g. {weekday: 0, n: 2} is the second Monday after the current date). n is optional and unbounded.\n2. Modify the WeekdayAnnotations validation method #The model_validator decorator can be used to hook into the validate process. model_validator has several modes which allow you to determine when and how customized validation occurs.\nIn this case, if the user passes in an instance of the dateutil._common.weekday type, we don\u0026rsquo;t need to do any more validation and will return it immediately. Otherwise, we want to run the normal Pydantic validation logic on the value that was passed in (i.e. make sure there is a weekday attribute that has an integer value between 0 and 6, etc.). The \u0026quot;wrap\u0026quot; mode passes a handler function as the second argument. The handler function calls the next validation step. In other words, it will call the normal model validation logic.\nThe return value from the handler will either be a WeekdayAnnotations object or a dateutil._common.weekday object. If it is a WeekdayAnnotations object, we need to convert it to a dateutil._common.weekday object.\nfrom pydantic import model_validator class WeekdayAnnotations(BaseModel): weekday: int = Field(ge=0, le=6) n: int | None = None @model_validator(mode=\u0026#34;wrap\u0026#34;) def _validate(value, handler) -\u0026gt; relativedelta: # if already dateutil._common.weekday instance, return it if isinstance(value, weekday): return value # otherwise run model validation which returns either a # a WeekdayAnnotations or dateutil._common.weekday object validated = handler(value) if isinstance(validated, weekday): return validated kwargs = {k: v for k, v in dict(validated).items() if v is not None} return weekday(**kwargs) For types that use __slots__, we also need to define a serialization function.\nNote: Defining a serialization function is only required for types that use __slots__. It is not required for most types that use the typical __dict__ structure. There isn\u0026rsquo;t a clear reason for this behavior as far as I can tell.\nDefining a serialization function is similar defining a validation function, except the serialization modes are different. We\u0026rsquo;ll use the \u0026quot;plain\u0026quot; mode which just replaces the built-in serializtion process the our custom function.\nfrom pydantic import model_serializer class WeekdayAnnotations(BaseModel): ... @model_serializer(mode=\u0026#34;plain\u0026#34;) def _serialize(self: weekday): return {\u0026#34;weekday\u0026#34;: self.weekday, \u0026#34;n\u0026#34;: self.n} Note that the first argument to our serialization function must be named self when using the @model_serializer decorator. The custom function just returns a dict with the weekday and n attributes.\n3. Add metadata to weekday #The final step is to tell Pydantic to use the validation, serialization, and schema generation logic contained in the WeekdayAnnotations model for the dateutil._common.weekday type. We can do this by adding metadata to the weekday type using Annotated.\nfrom typing import Annotated Weekday = Annotated[weekday, WeekdayAnnotations] If you examine Weekday\u0026rsquo;s metadata, you\u0026rsquo;ll see that it contains the WeekdayAnnotations model.\nWeekday.__metadata__ # (\u0026lt;class \u0026#39;__main__.WeekdayAnnotations\u0026#39;\u0026gt;,) Now we can use Weekday directly using pydantic.TypeAdapter.\nfrom pydantic import TypeAdapter WeekdayAdapter = TypeAdapter(Weekday) WeekdayAdapter.json_schema() # {\u0026#39;properties\u0026#39;: {\u0026#39;weekday\u0026#39;: {\u0026#39;maximum\u0026#39;: 6, \u0026#39;minimum\u0026#39;: 0, \u0026#39;title\u0026#39;: \u0026#39;Weekday\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;}, \u0026#39;n\u0026#39;: {\u0026#39;anyOf\u0026#39;: [{\u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;}, {\u0026#39;type\u0026#39;: \u0026#39;null\u0026#39;}], \u0026#39;default\u0026#39;: None, \u0026#39;title\u0026#39;: \u0026#39;N\u0026#39;}}, \u0026#39;required\u0026#39;: [\u0026#39;weekday\u0026#39;], \u0026#39;title\u0026#39;: \u0026#39;WeekdayAnnotations\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;object\u0026#39;} my_day = WeekdayAdapter.validate_python({\u0026#34;weekday\u0026#34;: 2, \u0026#34;n\u0026#34;: 2}) WeekdayAdapter.dump_python(my_day) # {\u0026#39;weekday\u0026#39;: 2, \u0026#39;n\u0026#39;: 2} Notice that values returned by WeekdayAdapter.validate_python are pure dateutil._common.weekday objects, they are not any sort of wrapped object.\ntype(my_day) == weekday # True Weekday can also be used as a field type in other Pydantic models. Again, the attribute types will be preserved as straight dateutil._common.weekday types. Since our validator accepts both dicts and dateutil._common.weekday objects, we can pass either into the model constructor as well.\nclass Foo(BaseModel): day: Weekday Foo(day={\u0026#34;weekday\u0026#34;: 2, \u0026#34;n\u0026#34;: 3}) # Foo(day=WE(+3)) Foo(day=weekday(2, 3)) # Foo(day=WE(+3)) type(Foo(day=weekday(2, 3)).day) == weekday # True Make relativedelta Pydantic compatible #Using the Weekday type we just created, we can follow the same process to make relativedelta compatible with Pydantic.\n1. Create a Pydantic model for relativedelta #If you look at the dateutil documentation, there are several additional fields that may be used in the relativedelta constructor which are converted in initialization. These are: weeks (converted to days), yearday and nlyearday (converted to day/month/leapdays). Finally, a relativedelta object can be created by passing two dates spanning the offset interval. The relativedelta object will be the difference between the two dates.\nThe full corresponding Pydantic model looks like this:\nfrom typing import Optional class RelativeDeltaAnnotation(BaseModel): years: int | None = None months: int | None = None days: int | None = None hours: int | None = None minutes: int | None = None seconds: int | None = None microseconds: int | None = None year: int | None = None # recommended way to avoid potential errors for compound types with constraints # https://docs.pydantic.dev/dev/concepts/fields/#numeric-constraints month: Optional[Annotated[int, Field(ge=1, le=12)]] = None day: Optional[Annotated[int, Field(ge=0, le=31)]] = None hour: Optional[Annotated[int, Field(ge=0, le=23)]] = None minute: Optional[Annotated[int, Field(ge=0, le=59)]] = None second: Optional[Annotated[int, Field(ge=0, le=59)]] = None microsecond: Optional[Annotated[int, Field(ge=0, le=999999)]] = None weekday: Weekday | None = None leapdays: int | None = None # validation only fields yearday: int | None = Field(None, exclude=True) nlyearday: int | None = Field(None, exclude=True) weeks: int | None = Field(None, exclude=True) dt1: int | None = Field(None, exclude=True) dt2: int | None = Field(None, exclude=True) Note that the validation-only fields have exclude=True so that they are not included in the serialization schema. They will still be included in the validation schema.\nfrom pydantic.json_schema import model_json_schema class SchemaTest(BaseModel): validation_only: None = Field(None, exclude=True) ## Validation schema includes validation-only fields print(json.dumps(model_json_schema(SchemaTest, mode=\u0026#39;validation\u0026#39;), indent=2)) # { # \u0026#34;properties\u0026#34;: { # \u0026#34;validation_only\u0026#34;: { # \u0026#34;default\u0026#34;: null, # \u0026#34;title\u0026#34;: \u0026#34;Validation Only\u0026#34;, # \u0026#34;type\u0026#34;: \u0026#34;null\u0026#34; # } # }, # \u0026#34;title\u0026#34;: \u0026#34;SchemaTest\u0026#34;, # \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34; # } ## Serialization schema does not include validation-only fields print(json.dumps(model_json_schema(SchemaTest, mode=\u0026#39;serialization\u0026#39;), indent=2)) # { # \u0026#34;properties\u0026#34;: {}, # \u0026#34;title\u0026#34;: \u0026#34;SchemaTest\u0026#34;, # \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34; # } 2. Modify RelativeDeltaAnnotation validation method #Similar to the WeekdayAnnotations model, we need to modify the validation function to return the third-party type instead of the Pydantic model.\nfrom pydantic_core import core_schema class RelativeDeltaAnnotation(BaseModel): ... @model_validator(mode=\u0026#34;wrap\u0026#34;) def _validate( value, handler: core_schema.ValidatorFunctionWrapHandler ) -\u0026gt; relativedelta: if isinstance(value, relativedelta): return value validated = handler(value) if isinstance(validated, relativedelta): return validated kwargs = {k: v for k, v in dict(validated).items() if v is not None} return relativedelta(**kwargs) Since the relativedelta does not use __slots__, we don\u0026rsquo;t need to define a custom serialization function.\n3. Add metadata to relativedelta #Finally, we can use Annotated again to add metadata to the relativedelta type.\nRelativeDelta = Annotated[relativedelta, RelativeDeltaAnnotation] Just like Weekday, RelativeDelta fields will resolve to pure relativedelta objects. RelativeDelta field annotations can be used in Pydantic models to provide full validation, serialization, and schema generation support while the actual model attributes remain relativedelta objects.\nfrom datetime import date from dateutil.relativedelta import TU from datetime import date class RecurringPayment(BaseModel): amount: float origination: date frequency: RelativeDelta periods: int def payments(self): for i in range(self.periods): yield self.origination + self.frequency * i mortgage = RecurringPayment(amount=100.0, origination=date(2020, 1, 1), frequency=relativedelta(months=1), periods=12) list(mortgage.payments()) # [datetime.date(2020, 1, 1), datetime.date(2020, 2, 1), ...] mortgage.frequency # relativedelta(months=+1) type(mortgage.frequency) # \u0026lt;class \u0026#39;dateutil.relativedelta.relativedelta\u0026#39;\u0026gt; mortgage.model_dump_json() # \u0026#39;{\u0026#34;amount\u0026#34;:100.0,\u0026#34;origination\u0026#34;:\u0026#34;2020-01-01\u0026#34;,\u0026#34;frequency\u0026#34;:{\u0026#34;years\u0026#34;:0,\u0026#34;months\u0026#34;:1,\u0026#34;days\u0026#34;:0,\u0026#34;leapdays\u0026#34;:0,\u0026#34;hours\u0026#34;:0,\u0026#34;minutes\u0026#34;:0,\u0026#34;seconds\u0026#34;:0,\u0026#34;microseconds\u0026#34;:0,\u0026#34;year\u0026#34;:null,\u0026#34;month\u0026#34;:null,\u0026#34;day\u0026#34;:null,\u0026#34;hour\u0026#34;:null,\u0026#34;minute\u0026#34;:null,\u0026#34;second\u0026#34;:null,\u0026#34;microsecond\u0026#34;:null,\u0026#34;weekday\u0026#34;:null},\u0026#34;periods\u0026#34;:12}\u0026#39; RecurringPayment.model_json_schema(mode=\u0026#34;serialization\u0026#34;) #{\u0026#39;$defs\u0026#39;: {\u0026#39;RelativeDeltaAnnotation\u0026#39;: {\u0026#39;properties\u0026#39;: {\u0026#39;years\u0026#39;: {\u0026#39;anyOf\u0026#39;: [{\u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;}, {\u0026#39;type\u0026#39;: \u0026#39;null\u0026#39;}], \u0026#39;default\u0026#39;: None, \u0026#39;title\u0026#39;: \u0026#39;Years\u0026#39;}, \u0026#39;months\u0026#39;: {\u0026#39;anyOf\u0026#39;: [{\u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;}, {\u0026#39;type\u0026#39;: \u0026#39;null\u0026#39;}], \u0026#39;default\u0026#39;: None, \u0026#39;title\u0026#39;: \u0026#39;Months\u0026#39;}, \u0026#39;days\u0026#39;: {\u0026#39;anyOf\u0026#39;: [{\u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;}, {\u0026#39;type\u0026#39;: \u0026#39;null\u0026#39;}], \u0026#39;default\u0026#39;: None, \u0026#39;title\u0026#39;: \u0026#39;Days\u0026#39;}, \u0026#39;hours\u0026#39;: {\u0026#39;anyOf\u0026#39;: [{\u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;}, {\u0026#39;type\u0026#39;: \u0026#39;null\u0026#39;}], \u0026#39;default\u0026#39;: None, \u0026#39;title\u0026#39;: \u0026#39;Hours\u0026#39;}, \u0026#39;minutes\u0026#39;: {\u0026#39;anyOf\u0026#39;: [{\u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;}, {\u0026#39;type\u0026#39;: \u0026#39;null\u0026#39;}], \u0026#39;default\u0026#39;: None, \u0026#39;title\u0026#39;: \u0026#39;Minutes\u0026#39;}, \u0026#39;seconds\u0026#39;: {\u0026#39;anyOf\u0026#39;: [{\u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;}, {\u0026#39;type\u0026#39;: \u0026#39;null\u0026#39;}], \u0026#39;default\u0026#39;: None, \u0026#39;title\u0026#39;: \u0026#39;Seconds\u0026#39;}, \u0026#39;microseconds\u0026#39;: {\u0026#39;anyOf\u0026#39;: [{\u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;}, {\u0026#39;type\u0026#39;: \u0026#39;null\u0026#39;}], \u0026#39;default\u0026#39;: None, \u0026#39;title\u0026#39;: \u0026#39;Microseconds\u0026#39;}, \u0026#39;year\u0026#39;: {\u0026#39;anyOf\u0026#39;: [{\u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;}, {\u0026#39;type\u0026#39;: \u0026#39;null\u0026#39;}], \u0026#39;default\u0026#39;: None, \u0026#39;title\u0026#39;: \u0026#39;Year\u0026#39;}, \u0026#39;month\u0026#39;: {\u0026#39;anyOf\u0026#39;: [{\u0026#39;maximum\u0026#39;: 12, \u0026#39;minimum\u0026#39;: 1, \u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;}, {\u0026#39;type\u0026#39;: \u0026#39;null\u0026#39;}], \u0026#39;default\u0026#39;: None, \u0026#39;title\u0026#39;: \u0026#39;Month\u0026#39;}, \u0026#39;day\u0026#39;: {\u0026#39;anyOf\u0026#39;: [{\u0026#39;maximum\u0026#39;: 31, \u0026#39;minimum\u0026#39;: 0, \u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;}, {\u0026#39;type\u0026#39;: \u0026#39;null\u0026#39;}], \u0026#39;default\u0026#39;: None, \u0026#39;title\u0026#39;: \u0026#39;Day\u0026#39;}, \u0026#39;hour\u0026#39;: {\u0026#39;anyOf\u0026#39;: [{\u0026#39;maximum\u0026#39;: 23, \u0026#39;minimum\u0026#39;: 0, \u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;}, {\u0026#39;type\u0026#39;: \u0026#39;null\u0026#39;}], \u0026#39;default\u0026#39;: None, \u0026#39;title\u0026#39;: \u0026#39;Hour\u0026#39;}, \u0026#39;minute\u0026#39;: {\u0026#39;anyOf\u0026#39;: [{\u0026#39;maximum\u0026#39;: 59, \u0026#39;minimum\u0026#39;: 0, \u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;}, {\u0026#39;type\u0026#39;: \u0026#39;null\u0026#39;}], \u0026#39;default\u0026#39;: None, \u0026#39;title\u0026#39;: \u0026#39;Minute\u0026#39;}, \u0026#39;second\u0026#39;: {\u0026#39;anyOf\u0026#39;: [{\u0026#39;maximum\u0026#39;: 59, \u0026#39;minimum\u0026#39;: 0, \u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;}, {\u0026#39;type\u0026#39;: \u0026#39;null\u0026#39;}], \u0026#39;default\u0026#39;: None, \u0026#39;title\u0026#39;: \u0026#39;Second\u0026#39;}, \u0026#39;microsecond\u0026#39;: {\u0026#39;anyOf\u0026#39;: [{\u0026#39;maximum\u0026#39;: 999999, \u0026#39;minimum\u0026#39;: 0, \u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;}, {\u0026#39;type\u0026#39;: \u0026#39;null\u0026#39;}], \u0026#39;default\u0026#39;: None, \u0026#39;title\u0026#39;: \u0026#39;Microsecond\u0026#39;}, \u0026#39;weekday\u0026#39;: {\u0026#39;anyOf\u0026#39;: [{\u0026#39;$ref\u0026#39;: \u0026#39;#/$defs/WeekdayAnnotations\u0026#39;}, {\u0026#39;type\u0026#39;: \u0026#39;null\u0026#39;}], \u0026#39;default\u0026#39;: None}, \u0026#39;leapdays\u0026#39;: {\u0026#39;anyOf\u0026#39;: [{\u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;}, {\u0026#39;type\u0026#39;: \u0026#39;null\u0026#39;}], \u0026#39;default\u0026#39;: None, \u0026#39;title\u0026#39;: \u0026#39;Leapdays\u0026#39;}}, \u0026#39;title\u0026#39;: \u0026#39;RelativeDeltaAnnotation\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;object\u0026#39;}, \u0026#39;WeekdayAnnotations\u0026#39;: {\u0026#39;title\u0026#39;: \u0026#39;WeekdayAnnotations\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;object\u0026#39;}}, \u0026#39;properties\u0026#39;: {\u0026#39;amount\u0026#39;: {\u0026#39;title\u0026#39;: \u0026#39;Amount\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;number\u0026#39;}, \u0026#39;origination\u0026#39;: {\u0026#39;format\u0026#39;: \u0026#39;date\u0026#39;, \u0026#39;title\u0026#39;: \u0026#39;Origination\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;string\u0026#39;}, \u0026#39;frequency\u0026#39;: {\u0026#39;$ref\u0026#39;: \u0026#39;#/$defs/RelativeDeltaAnnotation\u0026#39;}, \u0026#39;periods\u0026#39;: {\u0026#39;title\u0026#39;: \u0026#39;Periods\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;integer\u0026#39;}}, \u0026#39;required\u0026#39;: [\u0026#39;amount\u0026#39;, \u0026#39;origination\u0026#39;, \u0026#39;frequency\u0026#39;, \u0026#39;periods\u0026#39;], \u0026#39;title\u0026#39;: \u0026#39;RecurringPayment\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;object\u0026#39;} Alternative approaches #This approach uses the @model_validator decorator to provide an updated validation function that returns the third-party type. The Pydantic documentation uses a different hook into the validation process (__get_pydantic_core_schema__). The documentation approach requires you to define the core_schema.CoreSchema response directly.\nIf the third-party type can be easily constructed from the core_schema helper functions (e.g. core_schema.int_schema/str_schema/etc.) or if you need ultimate flexibility in defining the schema, then __get_pydantic_core_schema__ approach may be better. However, if the third-party type has many attributes like in the case of relativedelta, I find it more natural to define the shape of the schema using common BaseModel field annotations and overriding using the @model_validator decorator. These approaches aren\u0026rsquo;t mutually exclisive and can be used together.\nThe example in the documentation also defines a __get_pydantic_json_schema__ function. Unless you want to change the JSON schema (for example, to add examples), this function is not required.\n","date":"13 December 2023","permalink":"/posts/pydantic-with-third-party-types/","section":"Posts","summary":"Validating and structuring data models with Pydantic is a breeze, until you need to use incompatible third-party data types.","title":"Pydantic With Third Party Types"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"}]